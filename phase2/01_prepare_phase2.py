#!/usr/bin/env python3
"""
01_prepare_phase2.py
- Veri setlerini yÃ¼kler (CICIDS2017, UNSW-NB15),
- Temizler, kategorikleri one-hot yapar,
- MinMax ile Ã¶lÃ§ekler,
- AÅŸaÄŸÄ±daki dosyalarÄ± Ã¼retir:
    data/{DS}_processed.npy
    models/{DS}_scaler.pkl
    models/{DS}_columns.csv
- BÃ¼yÃ¼k dosyalar iÃ§in MAX_SAMPLES ile Ã¶rnekleme yapar.
"""
import os
import glob
import pickle
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

MAX_SAMPLES = 200_000
os.makedirs("models", exist_ok=True)
os.makedirs("data", exist_ok=True)

def load_cicids2017():
    base = "data/CICIDS2017"
    if not os.path.exists(base):
        print("âŒ CICIDS2017 klasÃ¶rÃ¼ bulunamadÄ±:", base); return None
    files = glob.glob(os.path.join(base, "*.csv"))
    if not files:
        print("âŒ CICIDS2017 CSV bulunamadÄ±."); return None

    # EÄŸer tek dosya varsa tamam, deÄŸilse parÃ§alayarak oku
    if len(files) == 1:
        print("âœ“ CICIDS2017 tek dosya bulundu:", os.path.basename(files[0]))
        df = pd.read_csv(files[0], nrows=MAX_SAMPLES)
        df.columns = df.columns.str.strip()
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)
        return df

    print(f"âœ“ CICIDS2017 Ã§oklu dosya ({len(files)}) â€” parÃ§a parÃ§a okunuyor.")
    per_file = max(1, int(MAX_SAMPLES / len(files)))
    chunks = []
    for f in files:
        try:
            c = pd.read_csv(f, nrows=per_file)
            c.columns = c.columns.str.strip()
            chunks.append(c)
        except Exception as e:
            print("Dosya okunamadÄ±:", f, e)
    if not chunks:
        return None
    df = pd.concat(chunks, ignore_index=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)
    return df

def load_unsw_nb15():
    base = "data/UNSW-NB15"
    if not os.path.exists(base):
        print("âŒ UNSW-NB15 klasÃ¶rÃ¼ bulunamadÄ±:", base); return None
    files = glob.glob(os.path.join(base, "*.csv"))
    if not files:
        print("âŒ UNSW-NB15 CSV bulunamadÄ±."); return None
    target = next((f for f in files if "training" in f.lower()), files[0])
    print("âœ“ UNSW-NB15 dosyasÄ± seÃ§ildi:", os.path.basename(target))
    df = pd.read_csv(target, nrows=MAX_SAMPLES)
    if "id" in df.columns: df.drop(columns=["id"], inplace=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)
    return df

def process_dataset(name, df):
    print(f"\nğŸš€ Ä°ÅŸleniyor: {name} | Ham shape: {df.shape}")

    # Sadece sayÄ±sal ve kategorik ayrÄ±mÄ±
    numerics = df.select_dtypes(include=[np.number]).columns.tolist()
    categoricals = df.select_dtypes(exclude=[np.number]).columns.tolist()

    # Kategorik kolonlar iÃ§in top-k koruma (top10)
    for col in categoricals:
        topk = df[col].value_counts().nlargest(10).index
        df[col] = df[col].apply(lambda x: x if x in topk else "Other")

    # One-hot
    df_encoded = pd.get_dummies(df, columns=categoricals, dummy_na=False)

    # Scale
    scaler = MinMaxScaler()
    arr = scaler.fit_transform(df_encoded.values.astype(np.float32))

    # Save
    np.save(f"data/{name}_processed.npy", arr)
    with open(f"models/{name}_scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    pd.Series(df_encoded.columns).to_csv(f"models/{name}_columns.csv", index=False, header=False)

    print(f"âœ” TamamlandÄ±: data/{name}_processed.npy | Shape: {arr.shape}")

if __name__ == "__main__":
    cic = load_cicids2017()
    if cic is not None:
        process_dataset("CICIDS2017", cic)
    unsw = load_unsw_nb15()
    if unsw is not None:
        process_dataset("UNSW-NB15", unsw)

